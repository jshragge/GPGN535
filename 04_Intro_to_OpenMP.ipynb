{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](figs/title.png)\n",
    "\n",
    "<h1><center>Module 04: Introduction to OpenMP</center></h1>\n",
    "\n",
    "\n",
    "This module is largely focussed on parallelization of algorithms using the OpenMP API. The building an understanding of multithreading execution model to speed up the algorithms (e.g., PDE solvers) you will develop throughout this course. \n",
    "\n",
    "As CPU speeds no longer improve as significantly as they did before, multicore systems are becoming more popular.\n",
    "To harness that power, it is becoming important for programmers to be knowledgeable in parallel programming — making a program execute multiple things simultaneously.\n",
    "\n",
    "This document attempts to give a quick introduction to OpenMP, a simple C/C++/Fortran compiler extension that allows to add parallelism into existing source code without significantly having to entirely rewrite it.  \n",
    "\n",
    "\n",
    "\n",
    "## What is OpenMP = <font color='red'>Open</font> <font color='red'>M</font>ulti-<font color='red'>P</font>arallelism\n",
    "\n",
    "OpenMP is an Application Program Interface (API) that may be used to explicitly direct multi-threaded, shared memory parallelism in C/C++ programs. It is not intrusive on the original serial code in that the OpenMP instructions are made in pragmas interpreted by the compiler.\n",
    "\n",
    "OpenMP is managed by the nonprofit technology consortium OpenMP Architecture Review Board (or OpenMP ARB), jointly defined by a group of major computer hardware and software vendors, including AMD, IBM, Intel, Cray, HP, Fujitsu, Nvidia, NEC, Red Hat, Texas Instruments, Oracle Corporation, and more.\n",
    "\n",
    "### Goals of OpenMP\n",
    "* **Standardization:**\n",
    "    *  Provide a standard among a variety of shared memory architectures/platforms\n",
    "    *  Jointly defined and endorsed by a group of major computer hardware and software vendors\n",
    "* **Lean and Mean:**\n",
    "    * Establish a simple and limited set of directives for programming shared memory machines.\n",
    "    * Significant parallelism can be implemented by using just 3 or 4 directives.\n",
    "* **Ease of Use:**\n",
    "    * Provide capability to incrementally parallelize a serial program, unlike message-passing libraries which typically require an all or nothing approach\n",
    "    * Provide the capability to implement both coarse-grain and fine-grain parallelism\n",
    "* **Portability:**\n",
    "    * The API is specified for C/C++ and Fortran\n",
    "    * Public forum for API and membership\n",
    "    * Most major platforms have been implemented including Unix/Linux platforms and Windows\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## OpenMP Programming Model\n",
    "\n",
    "### Shared Memory Model\n",
    "OpenMP is deisgned for multi-processor/core, shared memory machines.\n",
    "\n",
    "From a strictly hardware point of view, we see a typical shared-memory architecture where all processors can access to a common physical memory. If you have multiple processors, then those would be able to access exactly the same memory locations (see Figure 5). This architecture is very similar to what you have in your personal computers. Hence, developing high-performance parallel algorithms for shared-memory systems is straightforward and very similar to developing serial applications that you would normally run on your laptop. However, there are two big problems with this approach: 1) building CPUs with hundred thousands of cores is not possible for now; 2) all CPU-cores compete for access to memory over a shared bus. That's this kind of systems are not manufactured today. \n",
    "\n",
    "However, the idea of communicating directly through memory remains a useful programming abstraction. So in many systems, programmers use common programming techniques (e.g., OpenMP) to perform parallel tasks can directly access the same **\"virtual\"** memory regardless of where the physical memory actually exists. So, this programming model can be thought as an illusion that all memory is actually shared.\n",
    "\n",
    "<img src=\"figs/shared_mem.png\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "**<center>Figure 1. Shared memory architecture.</center>**\n",
    "\n",
    "**Because OpenMP is designed for shared memory parallel programming, it largely limited to single node parallelism. Typically, the number of processing elements (cores) on a node determine how much parallelism can be implemented.**\n",
    "\n",
    "\n",
    "### Program vs Process vs Thread\n",
    "#### Program\n",
    "A program is a set of instructions and associated data that resides on the disk and is loaded by the operating system to perform some task. An executable file or a python script file are examples of programs. In order to run a program, the operating system's kernel is first asked to create a new process, which is an environment in which a program executes.\n",
    "\n",
    "#### Process\n",
    "A process is a program in execution. A process is an execution environment that consists of instructions, user-data, and system-data segments, as well as lots of other resources such as CPU, memory, address-space, disk and network I/O acquired at runtime. A program can have several copies of it running at the same time but a process necessarily belongs to only one program.\n",
    "\n",
    "#### Thread\n",
    "Thread is the smallest unit of execution in a process. A thread simply executes instructions serially. A process can have multiple threads running as part of it. Usually, there would be some state associated with the process that is shared among all the threads and in turn each thread would have some state private to itself. The globally shared state amongst the threads of a process is visible and accessible to all the threads, and special attention needs to be paid when any thread tries to read or write to this global shared state. There are several constructs offered by various programming languages to guard and discipline the access to this global state, which we will go into further detail in upcoming lessons.\n",
    "\n",
    "<img src=\"figs/thread.png\" width=\"500\">\n",
    "\n",
    "**<center>Figure 2. A cartoon showing a process with three threads.</center>**\n",
    "\n",
    "**Processes do not share any resources amongst themselves whereas threads of a process can share the resources allocated to that particular process, including memory address space.**\n",
    "\n",
    "### Thread-based Parallelism\n",
    "\n",
    "OpenMP programs accomplish parallelism exclusively through the use of threads by utilizing the fork-join model of parallel execution. All OpenMP programs begin with a single master thread which executes sequentially until a parallel region is encountered, when it creates a team of parallel threads (FORK). When the team threads complete the parallel region, they synchronize and terminate, leaving only the master thread that executes sequentially (JOIN).\n",
    "\n",
    "<img src=\"figs/omp_thdbased.png\" width=\"1000\" align=\"center\"/>\n",
    "\n",
    "**<center>Figure 3. The fork-join model.</center>**\n",
    "\n",
    "### Data scoping\n",
    "Because OpenMP is a shared memory programming model, most data within a parallel region is shared by default. All threads in a parallel region can access this shared data simultaneously. OpenMP provides a way for the programmer to explicitly specify how data is \"scoped\" if the default shared scoping is not desired. \n",
    "\n",
    "### I/O\n",
    "OpenMP spicifies nothing about parallel I/O. This is particularly important if multiple threads attempt to read/write from the same file. If every threads conducts I/O to a different file. this issue is not as significant. It is entirely up to the programmer to ensure that I/O is conducted correctly within the context of a multi-threaded program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalability and Measuring Parallel Performance\n",
    "\n",
    "The real power of high-performance computer clusters comes from many processors in parallel to solve big problems. In parallel computing, a large number of processors work simultaneously to produce exceptional computational power and to significantly reduce the total computational time. In such scenarios, scalability or scaling is widely used to indicate the ability of hardware and software to deliver greater computational power when the amount of resources is increased. Scalability is an important aspect of HPC clusters demonstrating how the capacity of the whole system can be proportionally increased by adding more hardware. From a software point of view, scalability is sometimes referred to as parallelization efficiency&mdash;the ratio between the actual speedup and the ideal speedup obtained when using a certain number of processors.\n",
    "\n",
    "The speedup in parallel computing can be straightforwardly defined as\n",
    "\n",
    "$$\\text{Speedup}\\ (S_P) = \\frac{T_1}{T_P}, \\tag{1}$$\n",
    "\n",
    "where $T_1$ is the computational time for running the software using one processor, and $T_P$ is the computational time running the same software with $P$ processors. Ideally, we would like software to have a linear speedup that is equal to the number of processors (speedup = $P$), as that would mean that every processor would be contributing 100% of its computational power. Unfortunately, this is a very challenging goal for real applications to attain.\n",
    "\n",
    "The parallel efficiency is defined as the ratio of speedup to the number of processors. Efficiency measures the fraction of time for which a processor is usefully utilized and can be calculated as follows\n",
    "\n",
    "$$E=\\frac{S_P}{P}, \\tag{2}$$\n",
    "\n",
    "There are two basic ways to measure the parallel performance of a given application, depending on whether or not one is CPU-bound or memory-bound. These are referred to as strong and weak scaling, respectively.\n",
    "\n",
    "#### Amdahl's law and strong scaling\n",
    "Amdahl's law states that the speedup is limited by the fraction of the serial part of the software that is not amenable to parallelization. Amdahl's law can be formulated as follows\n",
    "\n",
    "$$S_p\\leq\\frac{1}{f+\\frac{1-f}{P}}, \\tag{3}$$\n",
    "\n",
    "where $f$ is the fraction of the execution time spent on the serial part, $1-f$ is the fraction of the execution time spent the part that can be parallelized, and $P$ is the number of processors. \n",
    "\n",
    "Amdahl's law states that the upper limit of speedup is determined by the serial fraction of the code. This law implicitly assumes that the problem size is fixed and the communication cost between processors are negligible. Problems for which these assumptions are reasonable can use this model for performance analysis. Such analysis is called **strong scaling**. \n",
    "\n",
    "\n",
    "**Q:** What is the maximum speedup of a code with 5% of its part sequential, executed on a 1000 processor cores? (What if it ran on 100 cores? What if the 5% is reduced to 1%? )\n",
    "\n",
    "#### Gustafson’s law and weak scaling\n",
    "Amdahl's law provides an upper limit of speedup for fixed-sized problems. However, sizes of problems scale with the amount of resources, and hence, using small amounts of resources for small problems and larger quantities of resources for big problems is a more reasonable approach. In real-life situations, we often want to keep the execution time constant and increase the problem size. We often want to keep the execution time constant and increase the problem size. \n",
    "\n",
    "Gustafson's law is based on the approximations that the parallel part scales linearly with the amount of resources, and that the serial part does not increase with respect to the size of the problem. The Gustafson's law can be formulated as follows\n",
    "\n",
    "$$\\text{Scaled Speedup}\\ S_p= f+(1-f)*P, \\tag{4}$$\n",
    "\n",
    "where $f$ is the fraction of the execution time spent on the serial part, $1-f$ is the fraction of the execution time spent the part that can be parallelized, and $P$ is the number of processors. According to the Gustafson's law, the scaled speedup increases linearly with respect to the number of processors and there is no upper limit for the scaled speedup. This is called **weak scaling**, where the scaled speedup is calculated based on the amount of work done for a scaled problem size. In other words, Gustafson's Law scales relative to the parallel fraction of a code and the serial fraction does not affect the scaling.\n",
    "\n",
    "#### Summary\n",
    "There are two ways of determining the scaling of a software: strong scaling and weak scaling. Some of the key points are summarized below:\n",
    "\n",
    "* Scalability is important for parallel computing  efficiency\n",
    "* Strong scaling concerns the speedup for a fixed problem size with respect to he number of processors, and is governed by Amdahl's law.\n",
    "* Weak scaling concerns the speedup for a scaled problem size with respect to the number of processors, and is governed by Gustafson's law.\n",
    "* The results of strong and weak scaling tests provide good indications for the best match between job size and the mount of resources that should be requested for a particular job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenMP API Overview\n",
    "OpenMP API has three distinct components:\n",
    "* Compiler directives\n",
    "* Runtime library routines\n",
    "* Environment variables\n",
    "\n",
    "As the new API versions are introduced, new directives, runtime library routines, and environment variables are added to the API. The programmer decides how to employ these components. In the simplest case, only a few of them are needed.\n",
    "\n",
    "#### Compiler Directives\n",
    "OpenMP compiler directives appear as comments in your source code and are ignored by compilers unless you tell them otherwise. This is usually done by specifying the appropriate compiler flag when compiling your source code.\n",
    "\n",
    "OpenMP directives are used for various purposes: \n",
    "* Spawning a parallel region \n",
    "* Dividing blocks of code among threads\n",
    "* Distributing loop iterations between threads\n",
    "* Serializing sections of code \n",
    "* Synchronization of work among threads\n",
    "\n",
    "\n",
    "The standard syntax for compiler directives are as given below:\n",
    "\n",
    "**sentinel**$\\;\\;\\;\\;\\;\\;$**directive-name**$\\;\\;\\;\\;\\;\\;$**[clause, ...]**\n",
    "\n",
    "For example:\n",
    "```c\n",
    "#pragma omp parallel default(shared) private(beta,pi)\n",
    "```\n",
    "\n",
    "#### Run-time Library Routines\n",
    "The OpenMP API includes several run-time library routines that are used for a variety of purposes: \n",
    "* Setting and querying the number of threads\n",
    "* Querying a thread's unique identifier (ID) \n",
    "* Querying if in a parallel regions, and at what level\n",
    "* Setting and querying nested parallelism\n",
    "\n",
    "For C/C++, all of the run-time library routines are actual functions. For example:\n",
    "\n",
    "```c\n",
    "#include <omp.h>\n",
    "int omp_get_num_threads(void)\n",
    "```\n",
    "\n",
    "Note that for C/C++, you need ot include the **<omp.h>** header file.\n",
    "\n",
    "#### Environment Variables\n",
    "OpenMP provides several environment variables for controlling the execution of parallel code at run-time.\n",
    "\n",
    "These environment variables can be used to control such things as:\n",
    "* Setting the number of threads\n",
    "* Specifying how loop interations are divided\n",
    "* Binding threads to processors\n",
    "* Enabling/disabling nested parallelism; setting the maximum levels of nested parallelism\n",
    "* Enabling/disabling dynamic threads\n",
    "* Setting thread wait policy\n",
    "\n",
    "You can set the OpenMP environment variables the same way you set any other environment variables. For example:\n",
    "\n",
    "```bash\n",
    "export OMP_NUM_THREADS=8\n",
    "```\n",
    "\n",
    "#### Example OpenMP Code Structure\n",
    "\n",
    "```c\n",
    "   #include <omp.h>\n",
    "\n",
    "   int main ()  {\n",
    "\n",
    "   int var1, var2, var3;\n",
    "\n",
    "   Serial code \n",
    "         .\n",
    "         .\n",
    "         .\n",
    "\n",
    "   Beginning of parallel region. Fork a team of threads.\n",
    "   Specify variable scoping \n",
    "\n",
    "   #pragma omp parallel private(var1, var2) shared(var3)\n",
    "      {\n",
    "\n",
    "      Parallel region executed by all threads \n",
    "                 .\n",
    "      Other OpenMP directives\n",
    "                 .\n",
    "      Run-time Library calls\n",
    "                 .\n",
    "      All threads join master thread and disband \n",
    "\n",
    "      }  \n",
    "\n",
    "   Resume serial code \n",
    "         .\n",
    "         .\n",
    "         .\n",
    "    \n",
    "       return 0;\n",
    "   }\n",
    "```\n",
    "\n",
    "#### Compiling OpenMP Programs\n",
    "OpenMP programs require you to use the appropriate compiler flag to \"turn on\" OpenMP compilations. For the gcc compiler you generally use in this class, the OpenMP compiler flag is **\"-fopenmp\"**.  \n",
    "\n",
    "## OpenMP Directives\n",
    "\n",
    "OpenMP directives is one of the core components of the OpenMP API. Parallel code with OpenMP marks, through a special directive, sections to be executed in parallel. The part of the code that is marked to run in parallel will cause threads to form. The main tread is the master thread. The slave threads all run in parallel and run the same code. Each thread executes the parallelized section of the code independently. When a thread finishes, it joins the master. When all threads finished, the master continues with code following the parallel section.\n",
    "\n",
    "### Parallel Region Construct\n",
    "In OpenMP framework, a parallel region is a block of code that will be executed by multiple threads. This is the fundamental OpenMP  parallel construct. When a thread reaches a parallel directive, it creates a team of threads and becomes the master of the team. The master is a member of that team and has thread number **0** within the that team. The compiler copies duplicates the source code starting from the beginning of the **parallel** region and all threads will execute that code. Once all threads finish their job and reach the end of the **parallel** region, they will either be destroyed or hit a barrier. After this point, only the master thread continues the execution.\n",
    "\n",
    "The syntax for the **\"parallel\"** construct is as follows\n",
    "\n",
    "```c\n",
    "#pragma omp parallel [clause ...]  newline \n",
    "                     if (scalar_expression) \n",
    "                     private (list) \n",
    "                     shared (list) \n",
    "                     default (shared | none) \n",
    "                     firstprivate (list) \n",
    "                     reduction (operator: list) \n",
    "                     copyin (list) \n",
    "                     num_threads (integer-expression)\n",
    "\n",
    " \n",
    "   structured_block\n",
    "```\n",
    "\n",
    "The number of threads in a parallel region can be determined by various ways. One can set the **\"num_threads\"** clause when defining a parallel region. Alternatively, the number of threads in a parallel region can be set by the OpenMP library function **\"omp_set_num_threads()\"** or setting the environment variable **\"OMP_NUM_THREADS\"** on your terminal shown as below:\n",
    "\n",
    "```bash\n",
    "export OMP_NUM_THREADS=8\n",
    "```\n",
    "\n",
    "When defining a parallel region, one should be aware that a parallel region must be a structure block that does not span multiple routines or code files. It is illegal to branch (goto) into ot out of a parallel region (goto should be literally illegal!). Additionally, only a single **num_threads** clause is permitted in a parallel regions definition. \n",
    "\n",
    "#### Hello, World \n",
    "\n",
    "Now, we know how to create a parallel region in OpenMP, we can go ahead and write our first parallel program.\n",
    "\n",
    "```c\n",
    "#include <omp.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    " \n",
    "int main (int argc, char *argv[]) {\n",
    "  int tid, nthreads;\n",
    "  #pragma omp parallel private(tid)\n",
    "  {\n",
    "    tid = omp_get_thread_num();\n",
    "    printf(\"Hello World from thread %d\\n\", tid);\n",
    "    #pragma omp barrier\n",
    "    if ( tid == 0 ) {\n",
    "      nthreads = omp_get_num_threads();\n",
    "      printf(\"There are %d threads\\n\",nthreads);\n",
    "    }\n",
    "  }\n",
    "  return EXIT_SUCCESS;\n",
    "}\n",
    "```\n",
    "\n",
    "OpenMP assumes data is shared among threads. However, we want each thread ot have their unique ID, so we can distinguish them. In the code above, we set the **\"tid\"** variable as **private** to each thread. When a variable is flagged as **private**, each threads generates a copy of that variable. Therefore, values of private variables can be different for each thread.\n",
    "\n",
    "### Work-Sharing Constructs\n",
    "A work-sharing construct divides the execution of an enclosed code region among the members of the team that encounter it. The work-sharing constructs do not launch new threads and use the threads that are created by the **parallel** construct. \n",
    "\n",
    "There are three types of work-sharing constructs: **\"for\"**, **\"sections\"**, and **\"single\"**. These constructs must be enclosed dynamically within a parallel region in for the directive to execute in parallel. All threads in a team must be able to encounter work-sharing constructs. When one uses successive work-sharing constructs, all members of a team must encounter these constructs in the same order.\n",
    "\n",
    "#### For directive\n",
    "The **for** directive specifies that the iterations of the loop immediately following it must be executed in parallel by the team. This assumes a parallel region has already been initiated, otherwise it executes in serial on a single processor. This approach represents a type of **\"data parallelism\"**.\n",
    "\n",
    "<img src=\"figs/omp_for.png\" width=\"250\" align=\"center\"/>\n",
    "\n",
    "**<center>Figure 4. The for directive in OpenMP.</center>**\n",
    "\n",
    "The syntax for the **for** directive is given below:\n",
    "\n",
    "```c\n",
    "#pragma omp for [clause ...]  newline \n",
    "                schedule (type [,chunk]) \n",
    "                ordered\n",
    "                private (list) \n",
    "                firstprivate (list) \n",
    "                lastprivate (list) \n",
    "                shared (list) \n",
    "                reduction (operator: list) \n",
    "                collapse (n) \n",
    "                nowait \n",
    "\n",
    "   for_loop\n",
    "```\n",
    "\n",
    "The explanations for each clause is given below.\n",
    "\n",
    "##### Schedule\n",
    "Here, the **\"schedule\"** clause describes how iterations of the loop are divided among the threads in the team. \n",
    "\n",
    "In **\"static\"** scheduling, loop iterations are divided into pieces of size chunk and then statically assigned to threads. If chunk is not specified, the iterations are evenly (if possible) divided contiguously among the threads.\n",
    "\n",
    "<img src=\"figs/omp_static.png\" width=\"750\" align=\"center\"/>\n",
    "\n",
    "**<center>Figure 5. The static scheduling in OpenMP.</center>**\n",
    "\n",
    "In **\"dynamic\"** scheduling, loop iterations are divided into pieces of size chunk, and dynamically scheduled among the threads; when a thread finishes one chunk, it is dynamically assigned another. The default chunk size is 1.\n",
    "\n",
    "<img src=\"figs/omp_dynamic.png\" width=\"750\" align=\"center\"/>\n",
    "\n",
    "**<center>Figure 6. The synamic scheduling in OpenMP.</center>**\n",
    "\n",
    "##### nowait\n",
    "In OpenMP, threads usually synchronize at the end of a parallel region, If one defines the **\"nowait\"** clause in **for** directive, the threads do not synchronize at the end of the parallel loop.\n",
    "\n",
    "##### ordered\n",
    "The **\"ordered\"** clause specifies that the iterations of the loop must be executed as they would be in a serial program.\n",
    "\n",
    "##### collapse\n",
    "\n",
    "The **\"collapse\"** clause specifies how many loops in a nested loop should be collapsed into one large iteration space and divided according to the schedule clause. The order of the iterations in the collapsed iteration space is determined as though they were executed sequentially. \n",
    "\n",
    "An example of collapsing a loop is given below:\n",
    "\n",
    "```c\n",
    "#pragma omp parallel for private(j) collapse(2)\n",
    "for (i = 0; i < 4; i++)\n",
    "    for (j = 0; j < 100; j++)\n",
    "```\n",
    "\n",
    "The for loop above can be manually collapsed as shown below:\n",
    " \n",
    " ```c\n",
    "#pragma omp parallel for\n",
    "for(int n=0; n<4*100; n++) {\n",
    "    int i = n/100; int j=n%100;\n",
    "```\n",
    "\n",
    "It is important that one is aware of the restrictions of the **for** directive, which can be summarized as below:\n",
    "* The loop iteration variable must be an integer and the loop control parameters must be the same for all threads.\n",
    "* Program correctness must not depend upon which thread executes a particular iteration.\n",
    "* It is illegal to branch out of a loop associated with a for directive.\n",
    "* The chunk size must be specified as a loop invariant integer expression, as there is no synchronization during its evaluation by different threads.\n",
    "* \n",
    "\n",
    "#### An example for the **for** directive\n",
    "\n",
    "The following code performs a parallel vector addition:\n",
    "\n",
    "```c\n",
    "#include <omp.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "#define N 1000\n",
    "#define CHUNKSIZE 100\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "\n",
    "\tint i, chunk;\n",
    "\tfloat a[N], b[N], c[N];\n",
    "\n",
    "\t/* initialize arrays */\n",
    "\tfor (i=0; i < N; i++) a[i] = b[i] = i * 1.0;\n",
    "\t\n",
    "\t/* set the chunk size */\n",
    "\tchunk = CHUNKSIZE;\n",
    "\n",
    "/* the beginning of the parallel region */\n",
    "#pragma omp parallel shared(a,b,c,chunk) private(i)\n",
    "{\n",
    "\t/* parallelize the following for loop */\n",
    "\t#pragma omp for schedule(dynamic,chunk) nowait\n",
    "\tfor (i=0; i < N; i++){\n",
    "\t\tc[i] = a[i] + b[i];\n",
    "\t}\n",
    "\n",
    "\t/* who are you ? */\t\n",
    "\tprintf(\"I am : %d\\n\",omp_get_thread_num());\n",
    "}   /* the end of the parallel region */\n",
    "return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "In the code above, arrays \"A\", \"B\", and \"C\" and the variable \"N\" will be shared by all threads. The variable \"i\" will be private to each thread; each thread will have its own unique copy. The iterations of the loop will be distributed dynamically in **chunk**-sized pieces. Finally, threads will not synchronize upon completing their individual pieces of work (**nowait**).\n",
    "\n",
    "#### Sections directive\n",
    "\n",
    "The **sections** is a non-iterative work-sharing construct. It specifies that the enclosed sections(s) of code are to be divided among the threads in the team.\n",
    "\n",
    "<img src=\"figs/omp_sections.png\" width=\"250\" align=\"center\"/>\n",
    "\n",
    "**<center>Figure 7. The sections directive in OpenMP.</center>**\n",
    "\n",
    "\n",
    "Independent **section** directives are nested within a **sections** directive. Each **section** is executed once by a thread in the team. Different sections may be executed by different threads. It is possible for a thread to execute more than one section if it is quick enough and the implementation permits such.\n",
    "\n",
    "The standard syntax for the **sections** directive is given below:\n",
    "\n",
    "```c\n",
    "\t\n",
    "#pragma omp sections [clause ...]  newline \n",
    "                     private (list) \n",
    "                     firstprivate (list) \n",
    "                     lastprivate (list) \n",
    "                     reduction (operator: list) \n",
    "                     nowait\n",
    "  {\n",
    "\n",
    "  #pragma omp section   newline \n",
    "\n",
    "     structured_block\n",
    "\n",
    "  #pragma omp section   newline \n",
    "\n",
    "     structured_block\n",
    "\n",
    "  }\n",
    "```\n",
    "There is an implied barrier at the end of a **sections** directive, unless the **nowait** clause is used.\n",
    "\n",
    "#### An example for the **sections** directive\n",
    "\n",
    "The code below shows an example of the usage of the **sctions** directive where a vector addition and multiplication are performed in parallel.\n",
    "\n",
    "```c\n",
    " #include <omp.h>\n",
    " #define N 1000\n",
    "\n",
    " int main(int argc, char *argv[]) {\n",
    "\n",
    " int i;\n",
    " float a[N], b[N], c[N], d[N];\n",
    "\n",
    " /* Some initializations */\n",
    " for (i=0; i < N; i++) {\n",
    "   a[i] = i * 1.5;\n",
    "   b[i] = i + 22.35;\n",
    "   }\n",
    "\n",
    " #pragma omp parallel shared(a,b,c,d) private(i)\n",
    "   {\n",
    "\n",
    "   #pragma omp sections nowait\n",
    "     {\n",
    "\n",
    "     #pragma omp section\n",
    "     for (i=0; i < N; i++)\n",
    "       c[i] = a[i] + b[i];\n",
    "\n",
    "     #pragma omp section\n",
    "     for (i=0; i < N; i++)\n",
    "       d[i] = a[i] * b[i];\n",
    "\n",
    "     }  /* end of sections */\n",
    "\n",
    "   }  /* end of parallel region */\n",
    "\n",
    "     return 0;\n",
    " }\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "#### Single directive\n",
    "\n",
    "The **single** directive specifies that the enclosed code is to be executed by only one thread in the team. Although the **single** directive may seem pointless in the first look, it may be useful when dealing with sections of code that are not thread safe (such as I/O). \n",
    "\n",
    "<img src=\"figs/omp_single.png\" width=\"250\" align=\"center\"/>\n",
    "\n",
    "**<center>Figure 8. The single directive in OpenMP.</center>**\n",
    "\n",
    "The syntax for the **single** directive is as given below:\n",
    "\n",
    "```c\n",
    "#pragma omp single [clause ...]  newline \n",
    "                   private (list) \n",
    "                   firstprivate (list) \n",
    "                   nowait\n",
    "\n",
    "     structured_block\n",
    "```\n",
    "\n",
    "The threads in the team that do not execute the **single** directive wait at the end of the enclosed code block, unless a **nowait** clause is specified.\n",
    "\n",
    "### Combined Parallel Work-Sharing Constructs\n",
    "\n",
    "OpenMP allows programmers to combine the parallel region directive with work-sharing constructs for convenience. Instead of creating a parallel region and work-sharing constructs within that region, you can just type:\n",
    "\n",
    "* parallel for\n",
    "* parallel sections\n",
    "\n",
    "The expressions above will launch threads, parallel region, and parallelize the code within the specified code block. \n",
    "\n",
    "For the most part, these directives behave identically to an individual PARALLEL directive being immediately followed by a separate work-sharing directive. Most of the rules, clauses and restrictions that apply to both directives are in effect. Therefor, you will mostly use these combined directives to parallelize your code. \n",
    "\n",
    "Below, you can find an example for combined parallel for directives:\n",
    "\n",
    "```c\n",
    "#include <omp.h>\n",
    " #define N       1000\n",
    " #define CHUNKSIZE   100\n",
    "\n",
    " int main(int argc, char *argv[]) {\n",
    "\n",
    " int i, chunk;\n",
    " float a[N], b[N], c[N];\n",
    "\n",
    " /* Some initializations */\n",
    " for (i=0; i < N; i++)\n",
    "   a[i] = b[i] = i * 1.0;\n",
    " chunk = CHUNKSIZE;\n",
    "\n",
    " #pragma omp parallel for \\\n",
    "   shared(a,b,c,chunk) private(i) \\\n",
    "   schedule(static,chunk)\n",
    "   for (i=0; i < N; i++)\n",
    "     c[i] = a[i] + b[i];\n",
    "    \n",
    "   return 0;\n",
    " }\n",
    "```\n",
    "\n",
    "### Data Scope (Data-Sharing) Attribute Clauses\n",
    "\n",
    "An important consideration for OpenMP programming is the understanding the data scoping. Because OpenMP is based upon the shared memory programming model, most variables are shared by default. However, you will extensively use data scope attribute clauses in conjunction with several directives (parallel, for, and sections) to control the scoping of enclosed variables. \n",
    "\n",
    "The **parallel**, **for**, and **sections** constructs allow programmer to define how and which data variables in the serial section of the program are transferred to the parallel regions of the program (and back). Additionally, as we saw in previous sections, you can define which variables will be visible to all threads in the parallel regions and which variables will be privately allocated to all threads.\n",
    "\n",
    "#### Private clause\n",
    "The **private** clause declares variables in its list to be private to each thread. \n",
    "\n",
    "The syntax for the **private** clause is as given below:\n",
    "\n",
    "```c\n",
    "private (list of variables separated by comma)\n",
    "```\n",
    "\n",
    "In your codes, you will almost always define loop index variables as private. \n",
    "\n",
    "Private variables behave as follows:\n",
    "* A new object of the same type is declared once for each thread in the team\n",
    "* All references to the original object are replaced with references to the new object\n",
    "* Should be assumed to be uninitialized for each thread\n",
    "\n",
    "#### Shared clause\n",
    "\n",
    "The **shared** clause declares variables in its list to be shared among all threads in the team.\n",
    "\n",
    "The syntax for the **shared** clause is the same as private clause:\n",
    "\n",
    "```c\n",
    "shared(list of variables separated by comma)\n",
    "```\n",
    "\n",
    "A shared variable exists in only one memory location and all threads can read or write to that address. It is the programmer's responsibility to ensure that multiple threads properly access **shared** variables.\n",
    "\n",
    "#### Default clause\n",
    "The **default** clause allows the user to specify a default scope for all variables in the lexical extent of any parallel region. You can use the **default** clause as follows: \n",
    "\n",
    "```c\n",
    "default(shared | private | none)\n",
    "```\n",
    "\n",
    "**Using \"none\" as a default requires that the programmer explicitly scope all variables.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
